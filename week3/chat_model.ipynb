{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b155a9f6",
   "metadata": {},
   "source": [
    "## OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477fbe66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI   ## instead of OpenAI,using ChatOpenAI \n",
    "# ctrl + click to see the documentation of ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c0d3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budapest\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI() ## create an instance of the ChatOpenAI model\n",
    "result = model.invoke(\"What is the capital of Hungary?\")\n",
    "# print(result)   additional output information like token usage, metadata\n",
    "\n",
    "print(result.content)  ## to get only the content of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a400db8",
   "metadata": {},
   "source": [
    "## Temperature : Creativity Parameter\n",
    "\n",
    "In LangChain, \"temperature\" is a parameter that controls the randomness and creativity of a language model's output. Higher values lead to more diverse and unpredictable responses, while lower values result in more focused and deterministic outputs.\n",
    "\n",
    "### How Temperature Works\n",
    "\n",
    "- **Low Temperature (e.g., 0-0.3):**  \n",
    "    The model consistently chooses the most probable words.  \n",
    "    *Use Case:* Factual summarization, code generation, or extracting information where accuracy is critical.\n",
    "\n",
    "- **Medium Temperature (e.g., 0.4-0.7):**  \n",
    "    Balances creativity and coherence.  \n",
    "    *Use Case:* General text generation or chatbot responses.\n",
    "\n",
    "- **High Temperature (e.g., 0.8-1.0+):**  \n",
    "    Allows the model to select less likely, more creative options.  \n",
    "    *Use Case:* Brainstorming, writing stories, or generating diverse ideas where novelty is prioritized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a24b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In circuits deep, where knowledge doth reside, a mind of metal, yet a heart denied.\n"
     ]
    }
   ],
   "source": [
    "model1 = ChatOpenAI(model=\"gpt-4\",temperature=0,max_completion_tokens=100)\n",
    "result1 = model1.invoke(\"write a poem about AI in the style of Shakespeare in 2 lines\")\n",
    "print(result1.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29dd783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In circuits deep, where knowledge doth reside, a mind of metal, yet a soul denied.\n",
      "Though metal hearts not beat, yet minds of silicon awake,\n",
      "In circuits deep, a consciousness, humanity's own make.\n"
     ]
    }
   ],
   "source": [
    "model2 = ChatOpenAI(model=\"gpt-4\",temperature=0.7) \n",
    "result2 = model2.invoke(\"write a poem about AI in the style of Shakespeare in 2 lines\")\n",
    "result1 = model1.invoke(\"write a poem about AI in the style of Shakespeare in 2 lines\")\n",
    "print(result1.content) \n",
    "print(result2.content) \n",
    "\n",
    "## temperture 0 = same output every time deterministic and chooses the most likely word\n",
    "## temperature 0.7 = more creative and varied output\n",
    "## try running the same cell multiple times to see the difference in output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358033f",
   "metadata": {},
   "source": [
    "### GOOGLE_GENERATIVE_AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c963c027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1997b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Hungary is **Budapest**.\n"
     ]
    }
   ],
   "source": [
    "model = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash\")  ## create an instance of the ChatGoogleGenerativeAI model\n",
    "result = model.invoke(\"What is the capital of Hungary?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d6679",
   "metadata": {},
   "source": [
    "## Open source Models:\n",
    "\n",
    "- Using HF Inference API\n",
    "- Running Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd022ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint ## endpoint for HF Inference API\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8f642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"deepseek-ai/DeepSeek-V3.1\",\n",
    "    task = \"text-generation\"\n",
    ")\n",
    "model = ChatHuggingFace(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab85b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Hungary is **Budapest**.\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(\"what is the capital of Hungary?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d66109",
   "metadata": {},
   "source": [
    "## Downloading and running Local Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08a7a1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task = 'text-generation',\n",
    "    pipeline_kwargs={'temperature': 0.8, 'max_new_tokens': 50}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is prime minister of hungary?\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"who is prime minister of hungary?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e474f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ramwani\\anaconda3\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "llm2 = HuggingFacePipeline.from_model_id(\n",
    "    model_id = 'Qwen/Qwen3Guard-Gen-0.6B',\n",
    "    task = 'text-generation',\n",
    "    pipeline_kwargs={'temperature': 0.8, 'max_new_tokens': 50}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8120b495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full form of AI? The full form of AI may be machine intelligence, or more generally, the ability of a machine to simulate human-like intelligence. It can be a wide range of artificial intelligence systems that are designed to perform general tasks such as problem solving, image recognition,\n"
     ]
    }
   ],
   "source": [
    "result = llm2.invoke(\"full form of AI?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f6b1124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is the prime minister of Hungary??\n",
      "President Viktor Orban\n",
      "\n",
      "President Viktor Orban has been the Prime Minister of Hungary since January 2019, succeeding Gyorányi Tamás as head of government, who served as Prime Minister from 2014 to \n"
     ]
    }
   ],
   "source": [
    "result = llm2.invoke(\"who is the prime minister of Hungary?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961fb257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id = 'google/flan-t5-large',\n",
    "    task = 'text2text-generation',\n",
    "    pipeline_kwargs={'temperature': 1.0, 'max_length': 250}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a664b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budapest\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke('what is the capital of Hungary?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28cada73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"who is the best footballer in the world?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72dac648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is the best footballer in the world? is it 1. Lionel Messi? 2. Cristiano Ronaldo? 3. Neymar? 4. Cristiano Ronaldo? Ronaldo? I don't like Ronaldo\n",
      "If I were to choose, the first two are Lionel Messi and Cristiano Ronaldo,\n"
     ]
    }
   ],
   "source": [
    "result = llm2.invoke(\"who is the best footballer in the world?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66c35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
